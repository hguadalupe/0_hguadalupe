
#BASIC packages
#Postgres, Anaconda, Jupyter, pipinstall, csvkit, Spark , Pyspark, findspark.

----------------------------------------------------------------
# Setting GITHUB remote connection:
## Appart from every instruction given by github, before you add/commit any file to an existing repository from your terminal, set the remote "origin":

> git remote set-url origin git@github.com:username/repo.git

#Clone all repositories from a user/org. Choose user or org, then type your username or orgname:
CNTX={users|orgs}; NAME={username|orgname}; PAGE=1
curl "https://api.github.com/$CNTX/$NAME/repos?page=$PAGE&per_page=100" |
  grep -e 'clone_url*' |
  cut -d \" -f 4 |
  xargs -L1 git clone

----------------------------------------------------------
#Setting-up Terminal. 
##Paste this in your ".bashrc" file.

###Terminal colors

parse_git_branch() {
     git branch 2> /dev/null | sed -e '/^[^*]/d' -e 's/* \(.*\)/(\1)/'
}

export PS1="\033[35m\]\d \t\[\033[m\] - \[\033[36m\]\u\[\033[m\] \[\e[32m\]\w \[\e[91m\]\$(parse_git_branch)\[\e[00m\] \n >> "

# make cursor visible
tput cnorm 
echo -ne "\e]12;green\a"

###Run and shut down commands for Anaconda and Jupyter.
alias jn='conda activate; jupyter notebook'
alias jnout='conda deactivate'
alias gitaddmit='git add .; git commit'

--------------------------------- 
#PYSPARK Route config
#open .bash_profile and paste this:

export SPARK_HOME="/your/home/directory/spark/python"
export PATH="$SPARK_HOME/bin:$PATH"

#then change your route of installation to your current spark_python route.


---------------
#Install findspark in JN. Open JUPYTER NOTEBOOK and type this (with "!" at the beginning):

!pip install findspark

#Now it should work in your jupyter notebook like this:
import findspark
findspark.init()
